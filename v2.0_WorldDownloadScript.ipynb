{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb1jcfkRAqO_"
   },
   "source": [
    "# World Airport Imagery Retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_KhTV-VBWnb"
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import folium\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "from sentinelsat import SentinelAPI, read_geojson, geojson_to_wkt \n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show\n",
    "import rasterio.mask\n",
    "import fiona\n",
    "import pyproj\n",
    "import gdal\n",
    "from datetime import datetime\n",
    "import pygc\n",
    "from io import StringIO\n",
    "import shutil\n",
    "import cv2\n",
    "from sklearn import cluster\n",
    "from osgeo import gdal, gdal_array\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = [r\"\\N\"]\n",
    "df0 = pd.read_csv(r'airport-codes_csv.csv',na_values = missing_values)\n",
    "list1,list2,a,b = [],[],[],[]\n",
    "for itema,itemb in zip(df0[\"iata_code\"],df0['iso_region']):\n",
    "    list1.append(itema)\n",
    "    list2.append(itemb)\n",
    "    \n",
    "for items in df0['coordinates']:\n",
    "    a.append(items.split(',')[0])\n",
    "    b.append(items.split(',')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "iatalist,iatabatch= [],[]\n",
    "batch_ranges = []\n",
    "batch_size = 500\n",
    "batch_number = 1\n",
    "missing_values = [r\"\\N\"]\n",
    "#df = pd.read_excel(r'190812_CompanyRelations.xlsx', sheet_name = 'Airport Data',na_values = missing_values)\n",
    "df = pd.read_csv(r'airportDB.csv')\n",
    "df_all = df\n",
    "#.sort_values(by = ['2018 Passengers'], ascending = False)\n",
    "#t = np.invert(df['2018 Passengers'].isnull())\n",
    "#df = df[t]\n",
    "#df = df.sort_values(by = [\"2018 Passengers\"], ascending = False) \n",
    "allNames = []\n",
    "for items,names in zip(df_all['IATA'],df_all['Name']):\n",
    "    iatabatch.append(items)\n",
    "    allNames.append(names)\n",
    "    \n",
    "def Remove(duplicate): \n",
    "    final_list = [] \n",
    "    for num in duplicate: \n",
    "        if num not in final_list: \n",
    "            final_list.append(num) \n",
    "    return final_list \n",
    "\n",
    "iatabatch = Remove(iatabatch)\n",
    "chunk_items = len(iatabatch) - 1\n",
    "\n",
    "for i in range(chunk_items):\n",
    "    batch_ranges.append(iatabatch[i*batch_size : (i+1)*batch_size])\n",
    "    \n",
    "batch_ranges\n",
    "\n",
    "for i in range(batch_size):\n",
    "    try:\n",
    "        iatalist.append(batch_ranges[batch_number -1][i])\n",
    "    except:\n",
    "        continue\n",
    "iatalist,allNames[:1000]\n",
    "air = dict(zip(iatalist,allNames[:1000]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('usairportDict.json', 'w') as json_file:\n",
    "    json.dump(air, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a database file of 6000+ airports containing their Latitude,Longitude,IATA codes etc. We use the consolidated list of interested we generated in the last block to query for the same from our Airports.dat file and prepare a new list of airport codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Airport:    \n",
    "    def __init__(self,name,city,con,iata,icao,lat,lon,alt):\n",
    "        self.name = name\n",
    "        self.city = city\n",
    "        self.country = con\n",
    "        self.iata = iata.upper() #3 letter\n",
    "        self.icao = icao.upper() #4 letter\n",
    "        self.lat = float(lat)\n",
    "        self.lon = float(lon)\n",
    "        self.alt = float(alt)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return ( str(self.iata+':'+self.name +':'+self.country))\n",
    "    def __repr__(self):\n",
    "        return ( str(self.iata+':'+self.name +':'+self.country))\n",
    "\n",
    "airportPath = \"airports.dat\"\n",
    "\n",
    "codes = {}\n",
    "apts = []\n",
    "with open(airportPath, encoding = \"utf8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        apt = Airport(row[1],row[2],row[3],\n",
    "                      row[4],row[5],\n",
    "                      float(row[6]),float(row[7]),float(row[8]))\n",
    "        codes[row[4].upper()] = apt\n",
    "        apts.append(apt)\n",
    "        \n",
    "#print(\"found %s airports\"%(len(apts)))\n",
    "X = codes[\"MEL\"].lon\n",
    "Y = codes[\"MEL\"].lat\n",
    "iata,iatalat,iatalon,notthere = [],[],[],[]\n",
    "for key in iatalist:\n",
    "    if key in list1:\n",
    "        iata.append(key)\n",
    "        #have changed codes --> list1 & codes[key].lat/lon --> a/b[list1.index(key)]\n",
    "        iatalon.append(a[list1.index(key)])\n",
    "        iatalat.append(b[list1.index(key)])\n",
    "    else : \n",
    "        notthere.append(key)\n",
    "    \n",
    "\n",
    "#print(iata[0],iatalat[0],iatalon[0])\n",
    "\n",
    "\n",
    "len(iatalat)\n",
    "\n",
    "#printing the entries that we have in our excel but not in our database(significantly improved results over the .dat file previously supplied)\n",
    "notthere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python graphic library, we try to virtually construct a geo-boundary translated from our required distance (typically around 10-20kms). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def latLonBoxByWandH(lat,lon,ew_width,ns_height):\n",
    "    lats, lons = [], []\n",
    "    #distance in m, az (in deg), lat (in deg), long (in deg)\n",
    "\n",
    "    res = pygc.great_circle(distance=ew_width/2, azimuth=90, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "\n",
    "    res = pygc.great_circle(distance=ns_height/2, azimuth=180, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "\n",
    "    res = pygc.great_circle(distance=ew_width, azimuth=270, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "\n",
    "    res = pygc.great_circle(distance=ns_height, azimuth=0, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "\n",
    "    res = pygc.great_circle(distance=ew_width, azimuth=90, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "    \n",
    "    return {'lats':lats,'lons':lons}\n",
    "\n",
    "\n",
    "#test the function\n",
    "#lengths in m\n",
    "ew_width = 10000\n",
    "ns_height = 10000\n",
    "loc_lat,loc_lon = [], []\n",
    "#loc_lat = [51.4775, 51.148056,1.359167]\n",
    "#loc_lon = [-0.461389, -0.190278, 103.989444]\n",
    "for i in range(0,len(iata)):\n",
    "    loc_lat.append(iatalat[i])\n",
    "    loc_lon.append(iatalon[i])\n",
    "    #print(iata[i])\n",
    "\n",
    "#loc_lon,loc_lat\n",
    "loc_lat = np.asfarray(loc_lat,float)\n",
    "loc_lon = np.asfarray(loc_lon,float)\n",
    "len(loc_lat),len(loc_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we construct the spatial boundary, we then try to project in on a basemap of the globe and use Folium to try and do a sanity check if our geo-boundaries actually fall under the area of our concern, in this case airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\fastai_v1\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Anaconda3\\envs\\fastai_v1\\lib\\site-packages\\pyproj\\crs\\crs.py:280: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  projstring = _prepare_from_string(projparams)\n"
     ]
    }
   ],
   "source": [
    "polygons,footprint =[], []\n",
    "\n",
    "for lat, lon in zip(loc_lat,loc_lon):\n",
    "\n",
    "    box = latLonBoxByWandH(lat,lon,ew_width,ns_height)\n",
    "    \n",
    "    polygon_geom = Polygon(zip(box['lons'], box['lats']))\n",
    "    footprint.append(polygon_geom)\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "    polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom])       \n",
    "    \n",
    "    polygons.append(polygon)\n",
    "\n",
    "\n",
    "pyproj.Proj(\"+init=epsg:4326\")\n",
    "\n",
    "m = folium.Map([loc_lat[0],loc_lon[0]], zoom_start=12,tiles = \"https://{s}.basemaps.cartocdn.com/light_nolabels/{z}/{x}/{y}{r}.png\",\n",
    "attr = '&copy; <a href=\"https://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors &copy; <a href=\"https://carto.com/attributions\">CARTO</a>')\n",
    "for polygon in polygons:\n",
    "    folium.GeoJson(polygon).add_to(m)\n",
    "    \n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('500_formetaArticle.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a query to Sentinel API using our boundary and specifying key parameters such as intersection, cloud cover, timeline of photo and the band of satellite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rs5TUxLbBCEr"
   },
   "outputs": [],
   "source": [
    "user = 'demi12395' \n",
    "password = 'Sutd1234' \n",
    "\n",
    "api = SentinelAPI(user, password, 'https://scihub.copernicus.eu/dhus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATL 16\n",
      "PEK 38\n",
      "DXB 35\n",
      "LAX 18\n",
      "HND 12\n",
      "ORD 6\n",
      "LHR 16\n",
      "HKG 5\n",
      "PVG 21\n",
      "CDG 31\n"
     ]
    }
   ],
   "source": [
    "apiq = [] \n",
    "for i in range(0,len(polygons)):\n",
    "    apiq.append(api.query(footprint[i],\n",
    "                 date = ('20200112','20200914'),\n",
    "                 platformname = 'Sentinel-2',\n",
    "                 processinglevel = 'Level-2A',\n",
    "                area_relation = ('Contains'),\n",
    "                 cloudcoverpercentage = (0,20)))\n",
    "for i in range(len(apiq)):\n",
    "    print(iatalist[i],len(apiq[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get a list of products (images in sentinel terminology), we try to work with the database and retrieve a consolidated list of product ID and title ID wherein the former is used as an unique identifier (to make another download query) in sentinel databse and latter gets stored as a folder in our local drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "1svWVHFUCsjL",
    "outputId": "6f56c155-631a-4451-902e-f35d0c6c65fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\fastai_v1\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "products_list, products_list_sorted,images,title,titlelist,best,bestlist =[],[], [], [], [] , [],[]\n",
    "for products in apiq:\n",
    "        products_list.append(api.to_geodataframe(products))\n",
    "        \n",
    "#Sorting the list of products within our array of locations for minimum cloudcover\n",
    "for products in products_list:\n",
    "    products_list_sorted.append(products.sort_values(['cloudcoverpercentage'],ascending = [False]))\n",
    "    \n",
    "\n",
    "for i in range(0,len(products_list_sorted)):\n",
    "        images.append(products_list_sorted[i].head(1))\n",
    "\n",
    "#for products in images: \n",
    "        #title.append(products.title)\n",
    "        #best.append(products.uuid)\n",
    "        \n",
    "for items in title: \n",
    "        items[18:][:99999]\n",
    "for products in images:\n",
    "    for i in range(0,len(products)):\n",
    "        title.append(products.title[i])\n",
    "        best.append(products.uuid[i])\n",
    "    bestlist.append(best)\n",
    "    titlelist.append(title)\n",
    "    title,best = [], []\n",
    "        \n",
    "#titlelist\n",
    "#bestlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "productsDF = pd.DataFrame(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6c53edc6-ebf9-4515-924c-09988d45f9c5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>S2A_MSIL2A_20200113T031101_N0213_R075_T50TMK_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>link</th>\n",
       "      <td>https://scihub.copernicus.eu/dhus/odata/v1/Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>link_alternative</th>\n",
       "      <td>https://scihub.copernicus.eu/dhus/odata/v1/Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>link_icon</th>\n",
       "      <td>https://scihub.copernicus.eu/dhus/odata/v1/Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>Date: 2020-01-13T03:11:01.024Z, Instrument: MS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beginposition</th>\n",
       "      <td>2020-01-13 03:11:01.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>endposition</th>\n",
       "      <td>2020-01-13 03:11:01.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ingestiondate</th>\n",
       "      <td>2020-01-13 12:07:11.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orbitnumber</th>\n",
       "      <td>23810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relativeorbitnumber</th>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vegetationpercentage</th>\n",
       "      <td>1.2571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>notvegetatedpercentage</th>\n",
       "      <td>31.0763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waterpercentage</th>\n",
       "      <td>2.04265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unclassifiedpercentage</th>\n",
       "      <td>15.7586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediumprobacloudspercentage</th>\n",
       "      <td>11.2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highprobacloudspercentage</th>\n",
       "      <td>2.11467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snowicepercentage</th>\n",
       "      <td>13.2651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cloudcoverpercentage</th>\n",
       "      <td>16.1754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level1cpdiidentifier</th>\n",
       "      <td>S2A_OPER_MSI_L1C_TL_EPAE_20200113T061224_A0238...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format</th>\n",
       "      <td>SAFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>processingbaseline</th>\n",
       "      <td>02.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platformname</th>\n",
       "      <td>Sentinel-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <td>S2A_MSIL2A_20200113T031101_N0213_R075_T50TMK_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instrumentname</th>\n",
       "      <td>Multi-Spectral Instrument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instrumentshortname</th>\n",
       "      <td>MSI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>1.13 GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s2datatakeid</th>\n",
       "      <td>GS2A_20200113T031101_023810_N02.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>producttype</th>\n",
       "      <td>S2MSI2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platformidentifier</th>\n",
       "      <td>2015-028A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orbitdirection</th>\n",
       "      <td>DESCENDING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platformserialidentifier</th>\n",
       "      <td>Sentinel-2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>processinglevel</th>\n",
       "      <td>Level-2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identifier</th>\n",
       "      <td>S2A_MSIL2A_20200113T031101_N0213_R075_T50TMK_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uuid</th>\n",
       "      <td>6c53edc6-ebf9-4515-924c-09988d45f9c5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geometry</th>\n",
       "      <td>(POLYGON ((115.8338683044463 39.65575257267611...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          6c53edc6-ebf9-4515-924c-09988d45f9c5\n",
       "title                        S2A_MSIL2A_20200113T031101_N0213_R075_T50TMK_2...\n",
       "link                         https://scihub.copernicus.eu/dhus/odata/v1/Pro...\n",
       "link_alternative             https://scihub.copernicus.eu/dhus/odata/v1/Pro...\n",
       "link_icon                    https://scihub.copernicus.eu/dhus/odata/v1/Pro...\n",
       "summary                      Date: 2020-01-13T03:11:01.024Z, Instrument: MS...\n",
       "beginposition                                       2020-01-13 03:11:01.024000\n",
       "endposition                                         2020-01-13 03:11:01.024000\n",
       "ingestiondate                                       2020-01-13 12:07:11.173000\n",
       "orbitnumber                                                              23810\n",
       "relativeorbitnumber                                                         75\n",
       "vegetationpercentage                                                    1.2571\n",
       "notvegetatedpercentage                                                 31.0763\n",
       "waterpercentage                                                        2.04265\n",
       "unclassifiedpercentage                                                 15.7586\n",
       "mediumprobacloudspercentage                                            11.2158\n",
       "highprobacloudspercentage                                              2.11467\n",
       "snowicepercentage                                                      13.2651\n",
       "cloudcoverpercentage                                                   16.1754\n",
       "level1cpdiidentifier         S2A_OPER_MSI_L1C_TL_EPAE_20200113T061224_A0238...\n",
       "format                                                                    SAFE\n",
       "processingbaseline                                                       02.13\n",
       "platformname                                                        Sentinel-2\n",
       "filename                     S2A_MSIL2A_20200113T031101_N0213_R075_T50TMK_2...\n",
       "instrumentname                                       Multi-Spectral Instrument\n",
       "instrumentshortname                                                        MSI\n",
       "size                                                                   1.13 GB\n",
       "s2datatakeid                                GS2A_20200113T031101_023810_N02.13\n",
       "producttype                                                            S2MSI2A\n",
       "platformidentifier                                                   2015-028A\n",
       "orbitdirection                                                      DESCENDING\n",
       "platformserialidentifier                                           Sentinel-2A\n",
       "processinglevel                                                       Level-2A\n",
       "identifier                   S2A_MSIL2A_20200113T031101_N0213_R075_T50TMK_2...\n",
       "uuid                                      6c53edc6-ebf9-4515-924c-09988d45f9c5\n",
       "geometry                     (POLYGON ((115.8338683044463 39.65575257267611..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "productsDF.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are trying to retrieve the datestamp of our product from the database and storing it seperately for our records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datestamp,stamp = [],[]\n",
    "for k in range(0,len(bestlist)):\n",
    "    for i in range(0,len(bestlist[k])):\n",
    "        d = products_list[k].beginposition[i].date().strftime(\"%Y%m%d\")\n",
    "        stamp.append(d)\n",
    "    datestamp.append(stamp)\n",
    "    stamp = []\n",
    "#products_list[1]\n",
    "#datestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kth best image available\n",
    "k = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodlist = bestlist\n",
    "#prodlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['S2A_MSIL2A_20200718T161911_N0214_R040_T16SGC_20200718T203123'],\n",
       " ['S2A_MSIL2A_20200113T031101_N0213_R075_T50TMK_20200113T071306'],\n",
       " ['S2B_MSIL2A_20200304T064749_N0214_R020_T40RCN_20200304T113926'],\n",
       " ['S2B_MSIL2A_20200203T183549_N0213_R027_T11SLT_20200203T210404'],\n",
       " ['S2A_MSIL2A_20200830T012701_N0214_R074_T54SUE_20200830T041851'],\n",
       " ['S2A_MSIL2A_20200724T163901_N0214_R126_T16TDM_20200724T205753'],\n",
       " ['S2A_MSIL2A_20200406T110621_N0214_R137_T30UXC_20200406T120807'],\n",
       " ['S2B_MSIL2A_20200822T025549_N0214_R032_T49QGE_20200822T050336'],\n",
       " ['S2A_MSIL2A_20200314T023551_N0214_R089_T51RUQ_20200314T045440'],\n",
       " ['S2B_MSIL2A_20200425T104619_N0214_R051_T31UDQ_20200425T134321']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titlelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtnIBMw-DEZW"
   },
   "source": [
    "Now we are trying to make an API call to download our product. Note that we are adding a method to extract the product and remove the parent file just to conservr memory as each product could take upto 1 GB of storage space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctk1ZhE_C-ma"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading product S2A_MSIL2A_20200718T161911_N0214_R040_T16SGC_20200718T203123 ... 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.19G/1.19G [03:52<00:00, 5.13MB/s]\n",
      "MD5 checksumming: 100%|████████████████████████████████████████████████████████████| 1.19G/1.19G [00:03<00:00, 369MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting  S2A_MSIL2A_20200718T161911_N0214_R040_T16SGC_20200718T203123\n",
      "Unzipping complete.. Now removing the file..\n",
      "Downloading product S2A_MSIL2A_20200113T031101_N0213_R075_T50TMK_20200113T071306 ... 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.21G/1.21G [05:24<00:00, 3.73MB/s]\n",
      "MD5 checksumming: 100%|████████████████████████████████████████████████████████████| 1.21G/1.21G [00:02<00:00, 415MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting  S2A_MSIL2A_20200113T031101_N0213_R075_T50TMK_20200113T071306\n",
      "Unzipping complete.. Now removing the file..\n",
      "Downloading product S2B_MSIL2A_20200304T064749_N0214_R020_T40RCN_20200304T113926 ... 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.19G/1.19G [08:31<00:00, 2.32MB/s]\n",
      "MD5 checksumming: 100%|████████████████████████████████████████████████████████████| 1.19G/1.19G [00:03<00:00, 383MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting  S2B_MSIL2A_20200304T064749_N0214_R020_T40RCN_20200304T113926\n",
      "Unzipping complete.. Now removing the file..\n",
      "Downloading product S2B_MSIL2A_20200203T183549_N0213_R027_T11SLT_20200203T210404 ... 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.06G/1.06G [06:55<00:00, 2.54MB/s]\n",
      "MD5 checksumming: 100%|████████████████████████████████████████████████████████████| 1.06G/1.06G [00:02<00:00, 400MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting  S2B_MSIL2A_20200203T183549_N0213_R027_T11SLT_20200203T210404\n",
      "Unzipping complete.. Now removing the file..\n",
      "Downloading product S2A_MSIL2A_20200830T012701_N0214_R074_T54SUE_20200830T041851 ... 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.20G/1.20G [02:55<00:00, 6.87MB/s]\n",
      "MD5 checksumming: 100%|████████████████████████████████████████████████████████████| 1.20G/1.20G [00:03<00:00, 337MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting  S2A_MSIL2A_20200830T012701_N0214_R074_T54SUE_20200830T041851\n",
      "Unzipping complete.. Now removing the file..\n",
      "Downloading product S2A_MSIL2A_20200724T163901_N0214_R126_T16TDM_20200724T205753 ... 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.14G/1.14G [02:33<00:00, 7.41MB/s]\n",
      "MD5 checksumming: 100%|████████████████████████████████████████████████████████████| 1.14G/1.14G [00:02<00:00, 402MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting  S2A_MSIL2A_20200724T163901_N0214_R126_T16TDM_20200724T205753\n",
      "Unzipping complete.. Now removing the file..\n",
      "Downloading product S2A_MSIL2A_20200406T110621_N0214_R137_T30UXC_20200406T120807 ... 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.16G/1.16G [02:37<00:00, 7.38MB/s]\n",
      "MD5 checksumming: 100%|████████████████████████████████████████████████████████████| 1.16G/1.16G [00:03<00:00, 373MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting  S2A_MSIL2A_20200406T110621_N0214_R137_T30UXC_20200406T120807\n",
      "Unzipping complete.. Now removing the file..\n",
      "Downloading product S2B_MSIL2A_20200822T025549_N0214_R032_T49QGE_20200822T050336 ... 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.01G/1.01G [04:09<00:00, 4.05MB/s]\n",
      "MD5 checksumming: 100%|████████████████████████████████████████████████████████████| 1.01G/1.01G [00:02<00:00, 397MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting  S2B_MSIL2A_20200822T025549_N0214_R032_T49QGE_20200822T050336\n",
      "Unzipping complete.. Now removing the file..\n",
      "Downloading product S2A_MSIL2A_20200314T023551_N0214_R089_T51RUQ_20200314T045440 ... 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.07G/1.07G [06:06<00:00, 2.93MB/s]\n",
      "MD5 checksumming: 100%|████████████████████████████████████████████████████████████| 1.07G/1.07G [00:02<00:00, 403MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting  S2A_MSIL2A_20200314T023551_N0214_R089_T51RUQ_20200314T045440\n",
      "Unzipping complete.. Now removing the file..\n",
      "Downloading product S2B_MSIL2A_20200425T104619_N0214_R051_T31UDQ_20200425T134321 ... 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.18G/1.18G [06:35<00:00, 2.97MB/s]\n",
      "MD5 checksumming: 100%|████████████████████████████████████████████████████████████| 1.18G/1.18G [00:02<00:00, 399MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting  S2B_MSIL2A_20200425T104619_N0214_R051_T31UDQ_20200425T134321\n",
      "Unzipping complete.. Now removing the file..\n"
     ]
    }
   ],
   "source": [
    "directory  = \"Products\"\n",
    "counter = 1\n",
    "for prods,titles in zip(prodlist,titlelist):\n",
    "    for items,items2 in zip(prods,titles):\n",
    "        print(\"Downloading product \" + items2 + \" ... \" + str(counter) + \"/\" + str(len(titlelist)))\n",
    "        counter+=1\n",
    "        api.download(items, directory_path = directory)\n",
    "        with ZipFile(\"Products/\" + items2 + '.zip', 'r') as zf:\n",
    "            print(\"extracting  \" + items2)\n",
    "            zf.extractall('Products/')\n",
    "            zf.close()  \n",
    "        print(\"Unzipping complete.. Now removing the file..\")\n",
    "        os.remove(\"Products/\" + items2 + '.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder,foldername,dire,allmapsinc,allmaps,diretemp =[], [], [],[],[],[]\n",
    "for titles in titlelist:\n",
    "    for items in titles:\n",
    "        d = os.listdir(\"Products/\" + items + \".SAFE/\" + \"GRANULE\")\n",
    "        folder.append(d[0])\n",
    "    foldername.append(folder)\n",
    "    folder = []\n",
    "foldername\n",
    "\n",
    "for titles,folders in zip(titlelist,foldername):\n",
    "    for items,names in zip(titles,folders):\n",
    "        maindir = \"Products/\" + items + \".SAFE/\" + \"GRANULE/\"  +str(names) + \"/IMG_DATA\" + \"/R10m/\" \n",
    "        allmapsinc.append(maindir)\n",
    "        diretemp.append(os.listdir(maindir))\n",
    "    allmaps.append(allmapsinc)\n",
    "    dire.append(diretemp)\n",
    "    allmapsinc,diretemp = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDITED BY SCJ\n",
    "templatemp,template = [],[]\n",
    "for k in range(0,len(dire)):\n",
    "    for i in range(0,len(dire[0])):\n",
    "        #templatemp.append(dire[k][i][-2][0:][:-4]) #SCJ removed as seems to rely on file order, doesn't work on mac\n",
    "        files = [a for a in dire[k][i] if \"_TCI_10m.jp2\" in a]\n",
    "        #print(files)\n",
    "        templatemp.append(files[0][:-4])   \n",
    "    template.append(templatemp)\n",
    "    templatemp = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "templatemp,template = [],[]\n",
    "for k in range(0,len(dire)):\n",
    "    for i in range(0,len(dire[0])):\n",
    "        templatemp.append(dire[k][i][-2][0:][:-4])\n",
    "    template.append(templatemp)\n",
    "    templatemp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamically retrieving the directory that we need for processing and rendering our image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmapscompleteinc,allmapscomplete = [],[]\n",
    "for k in range(0,len(prodlist)):\n",
    "    for i in range(0,len(prodlist[0])):\n",
    "        allmapscompleteinc.append(allmaps[k][i] + \"/\" + template[k][i] + \".jp2\")\n",
    "    allmapscomplete.append(allmapscompleteinc)\n",
    "    allmapscompleteinc = []\n",
    "#allmapscomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataArr = []\n",
    "for items in allmapscomplete:\n",
    "    data = rio.open(items[0])\n",
    "    #print(items)\n",
    "    #print(data.meta['crs'])\n",
    "    dataArr.append(data)\n",
    "    data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening a test image using Rasterio and checking its projection CRS and other metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nzy2A8MLDoX5"
   },
   "source": [
    "### Mask Satellite images\n",
    "\n",
    "Now we try to reproject our images into EPSG system in order to average out for all the different parts of earth. This process could take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Warping for  1 ATL\n",
      "Finished Warping for  2 PEK\n",
      "Finished Warping for  3 DXB\n",
      "Finished Warping for  4 LAX\n",
      "Finished Warping for  5 HND\n",
      "Finished Warping for  6 ORD\n",
      "Finished Warping for  7 LHR\n",
      "Finished Warping for  8 HKG\n",
      "Finished Warping for  9 PVG\n",
      "Finished Warping for  10 CDG\n"
     ]
    }
   ],
   "source": [
    "from osgeo import gdal\n",
    "filename,file = [],[]\n",
    "#gdoptions = gdal.WarpOptions(options = ['dstSRS = \"WGS84\"','xRes = 10', \"yRes = 10\",\" targetAlignedPixels = True\"])\n",
    "gdoptions = gdal.WarpOptions('dstSRS = WGS84')\n",
    "for k in range(0,len(allmaps)):\n",
    "    for i in range(0,len(allmaps[0])):\n",
    "        input_raster = gdal.Open(allmapscomplete[k][i])\n",
    "        output_raster = allmaps[k][i] + template[k][i] +\".tiff\"\n",
    "        file.append(output_raster)\n",
    "        options=['dstSRS = EPSG:4326','width = 10980', 'height = 10980']\n",
    "        gdal.Warp(output_raster,input_raster,srcSRS = str(dataArr[k].meta['crs']),dstSRS = 'epsg:4326',width = str(dataArr[k].width),height = str(dataArr[k].height))\n",
    "        print(\"Finished Warping for \" + \" \" + str(k + 1) + \" \" + iatalist[k] )\n",
    "    filename.append(file)\n",
    "    file = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:4326\n",
      "EPSG:4326\n",
      "EPSG:4326\n",
      "EPSG:4326\n",
      "EPSG:4326\n",
      "EPSG:4326\n",
      "EPSG:4326\n",
      "EPSG:4326\n",
      "EPSG:4326\n",
      "EPSG:4326\n"
     ]
    }
   ],
   "source": [
    "for items in filename:\n",
    "    data = rio.open(items[0])\n",
    "    #print(items)\n",
    "    print(data.meta['crs'])\n",
    "    data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nextly, we directly mask the image and write out the clipped photo in a seperate folder with our usual naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.mask import mask\n",
    "src_crs = []\n",
    "\n",
    "geoms = []\n",
    "for i in range(0,len(datestamp)):\n",
    "    geoms.append(polygons[i]['geometry'])\n",
    "    \n",
    "for k in range(0,len(prodlist[0])):\n",
    "    for i in range(0,len(prodlist)):\n",
    "        with rasterio.open(filename[i][k]) as src:\n",
    "                out_image, out_transform = mask(src, geoms[i], crop=True)\n",
    "                out_meta = src.meta.copy()\n",
    "\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                             \"height\": out_image.shape[1],\n",
    "                             \"width\": out_image.shape[2],\n",
    "                             \"transform\": out_transform})\n",
    "        try:\n",
    "            with rasterio.open(\"Source/S\" + \"_\" +  iatalist[i] + \"_\" + datestamp[i][k]  +\".tiff\", \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "            #os.remove(titlelist[k][i])\n",
    "            src.close()\n",
    "            dest.close()\n",
    "        except:\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "RS-Python.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "fastai v1",
   "language": "python",
   "name": "fastai_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
